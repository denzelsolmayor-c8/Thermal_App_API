from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.dialects.postgresql import insert as pg_insert

from database import Base, get_async_session

router = APIRouter()


class SheetData(BaseModel):
    sheet_name: str
    headers: List[str]
    data: List[List[Any]]
    created_at: str


class UploadPayload(BaseModel):
    id: str
    filename: str
    sheets: List[SheetData]


def get_model_mapping():
    return {
        "camera_configs": Base.classes.camera_configs,
        "camera_in_zone": Base.classes.camera_in_zone,
        "camera_presets": Base.classes.camera_presets,
        "zones": Base.classes.zones,
        "customer": Base.classes.customer,
        "temperatures": Base.classes.temperatures,
    }


insert_order = [
    "zones",
    "customer",
    "camera_configs",
    "camera_presets",
    "camera_in_zone",
    "temperatures",
]


@router.post("/api/upload-file-data")
async def upload_file_data(
    payload: UploadPayload,
    session: AsyncSession = Depends(get_async_session)
):
    models = get_model_mapping()
    result: Dict[str, List[Dict[str, Any]]] = {name: [] for name in models}

    # 1) COLUMN-BASED ROUTING: for each row, try to match it to each table
    for sheet in payload.sheets:
        print(
            f"[DEBUG] Processing sheet: {sheet.sheet_name} with {len(sheet.data)} rows")
        for row in sheet.data:
            row_dict = dict(zip(sheet.headers, row))

            # Skip alarm descriptions
            desc = row_dict.get("description", "")
            if isinstance(desc, str) and "alarm" in desc.lower():
                continue

            for table_name, model in models.items():
                cols = set(model.__table__.columns.keys())
                # exclude autogenerated columns
                non_auto = cols - {"rel_id", "id"}

                entry = {k: v for k, v in row_dict.items() if k in cols}

                if non_auto.issubset(entry.keys()):
                    result[table_name].append(entry)
                else:
                    missing = non_auto - entry.keys()
                    if row_dict:  # only log non-empty rows
                        print(
                            f"[SKIP] Row for table '{table_name}' missing required fields: {missing}")

    print("[DEBUG] Prepared insert counts:", {
          k: len(v) for k, v in result.items()})

    # 2) BULK INSERT each table in dependency order
    for table_name in insert_order:
        entries = result.get(table_name)
        if not entries:
            print(f"[DEBUG] Skipping insert for '{table_name}' â€” no entries.")
            continue
        model = models[table_name]

        # Optional type conversions
        for e in entries:
            if table_name in ("camera_presets", "camera_in_zone"):
                if "preset_number" in e:
                    e["preset_number"] = int(e["preset_number"])
            if table_name == "temperatures":
                if "point_in_preset" in e:
                    e["point_in_preset"] = int(e["point_in_preset"])
                if "preset_number" in e:
                    e["preset_number"] = int(e["preset_number"])

        try:
            stmt = pg_insert(model).values(entries).on_conflict_do_nothing()
            await session.execute(stmt)
            print(f"[DEBUG] Inserted into '{table_name}': {len(entries)} rows")
        except Exception as exc:
            print(f"[ERROR] {table_name} bulk insert failed:", exc)

    try:
        await session.commit()
    except Exception as exc:
        await session.rollback()
        raise HTTPException(500, f"Commit failed: {exc}")

    return {
        "message": "Upload complete",
        "record_count": {t: len(v) for t, v in result.items()}
    }
